/<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Transferring robot skills to enhance general-purpose object manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Transferring robot skills to enhance general-purpose object manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Sichao Liu<sup>1,2,3</sup>,</span>
            <span class="author-block">
              Lihui Wang</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>KTH Royal Institute of Technology,</span>
            <span class="author-block"><sup>2</sup>University of Cambridge,</span>
            <span class="author-block"><sup>3</sup> École Polytechnique Fédérale de Lausanne</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a novel pose estimation approach aimed at determining the 6D pose of objects with precision, including accurate bounding boxes and textual labels. This method concurrently establishes precise and robust pose estimates for multiple objects while enabling real-time tracking of these poses. Leveraging a visual servoing system, our approach empowers robots to promptly measure and track object poses in real-time, facilitating dynamic planning of robot arm trajectories to approach targets. Moreover, our developed methodology seamlessly integrates with Language Understanding Models (LLMs), allowing for intuitive natural language instructions such as 'please grasp the mayo.' Consequently, the robot comprehends and executes such instructions to manipulate objects effectively.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline width="80%">
            <source src="./static/videos/mayo1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline width="80%">
            <source src="./static/videos/mayo2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline width="80%">
            <source src="./static/videos/mayo3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline width="80%">
            <source src="./static/videos/mayo4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline width="80%">
            <source src="./static/videos/mayo5.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

          <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Robot skills of object manipultion</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">6D pose supported robot grasping</h3>
        <div class="content has-text-justified">
          <p>
            We achieved the 6D pose estimation of multiple known objects within a scene using a series of input images, generating corresponding text labels and bounding boxes for each object. Employing a pre-trained pose estimation model, RGB-D images captured by a meticulously calibrated Azure camera are utilized to construct precise 6D pose information. As illustrated on the left, text-based instructions such as 'pick up the mayo' direct the robot to grasp the specified 'mayo' object, followed by 'place the mayo in the blue-marked box,' guiding the robot to accurately position the object at the designated target. On the right, another instance showcases the robot's proficiency in executing picking-and-placing operations
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-3 has-text-centered">
            <video id="matting-video" autoplay controls muted loop playsinline width="300%">
              <source src="./static/videos/mayo.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="column is-3 has-text-centered">
            <video id="matting-video" autoplay controls muted loop playsinline width="300%">
              <source src="./static/videos/catch.mp4"
                      type="video/mp4">
            </video>
          </div>
        
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Interpolating. -->
        <h3 class="title is-4">Robust pose tracking for object manipulation</h3>
        <div class="content has-text-justified">
          <p>
            We realised robust and real-time pose tracking of the object, and it can enable the robots to handle complex maanipulation tasks, especially in a dynamic situation. 
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-3 has-text-centered">
            <video id="matting-video" autoplay controls muted loop playsinline width="150%">
              <source src="./static/videos/posetrack.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="column is-3 has-text-centered">
            <video id="matting-video" autoplay controls muted loop playsinline width="150%">
              <source src="./static/videos/posetrack1.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="column is-3 has-text-centered">
            <video id="matting-video" autoplay controls muted loop playsinline width="150%">
              <source src="./static/videos/posetrack2.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">LLM-driven general-purpose object manipulation</h3>
        <div class="content has-text-justified">
          <p>
            In scenarios where multiple identical objects are present, implicit instructions like 'pick-and-place mayo' can often lead to confusion for the robot, lacking clear rules or manipulation policies. In this context, we leverage a Language Understanding Model (LLM) to provide precise text-based instructions for the robot to identify and handle the correct object. Specifically, an input image depicting the scene and its objects is processed by the LLM, enabling object recognition and counting. A natural language command such as 'please pick the mayo in the middle' guides the robot to select the appropriate object.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop playsinline width="45%">
            <source src="./static/videos/mayo any.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->


       


      </div>
    </div>
    <!--/ Animation. -->
  
  </div>
</section>





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{liu2024transfer,
  author    = {Liu, Sichao and Wang, Lihui},
  title     = {Transferring robot skills to enhance general-purpose object manipulation},
  year={2024}
}</code></pre>
  </div>
</section>


<!-- <footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

</body>
</html>
